---
title: "Case Study Question Set 3"
author: "Matthew Benningfield"
output: pdf_document
---

```{r}
library(ggplot2) # Visualization
library(data.table) # Manipulation
library(readxl) # Reading in data
library(reshape) # Help with Correlation Matrix

# Read in the data sets
MLB_Pitchers <- read_excel("MLB_pitchers.xlsx")
Myrtle_Beach_Homes <- read_excel("Myrtle_BeachHomes.xlsx")
Philadelphia <- read_excel("Philadelphia.xlsx")
Production_Costs <- read_excel("Production_costs.xlsx")
Residual_Car_Values <- read_excel("Residual_value.xlsx")
```

```{r}
# Convert each data frame to data table for manipulation
setDT(MLB_Pitchers)
setDT(Myrtle_Beach_Homes)
setDT(Philadelphia)
setDT(Production_Costs)
setDT(Residual_Car_Values)
```

**----SOLUTIONS----**

**MLB Pitchers**

**1a, 1b, 1c, 1d**

```{r}
# Get correlation matrix of numeric variables (quantitative variables)
sapply(MLB_Pitchers, is.numeric)
cor(MLB_Pitchers[, which(sapply(MLB_Pitchers, is.numeric)), with = FALSE])

# Get the max correlation with our response variable "Wins"
correl <- cor(MLB_Pitchers[, which(sapply(MLB_Pitchers, is.numeric)), with = FALSE])

# Drop perfects
correl[correl == 1] <- NA

# Melt correl
correl <- na.omit(melt(correl))
correl_table <- setDT(correl[order(-abs(correl$value)),])
correl_table[X1 == "Wins" | X2 == "Wins"][2]
```

$y=\beta_0 + \beta_1x + \epsilon$

**2a, 2b, 2c**

**Solutions are incorrect for the intercept**

```{r}
model1 <- lm(Wins ~ ERA, data = MLB_Pitchers)
summary(model1)$coefficients
```

A one unit increase in ERA decreases the wins by 2.67

```{r}
cat("R-squared = ", summary(model1)$r.squared * 100, "%")
```

**3a, 3b, 3c**

```{r}
# Point prediction
predict(model1, data.frame(ERA = 4.5))

# Confidence interval
predict(model1, data.frame(ERA = 4.5), interval = "confidence", level = 0.90)

# Prediction interval
predict(model1, data.frame(ERA = 4.5), interval = "prediction", level = 0.90)
```

**4a, 4b, 4c**

```{r}
# Confidence interval for slope
# Does not include 0
confint(model1, "ERA", level = 0.95)
```

$H_0: \beta_1 = 0$

$H_a: \beta_1 \ne 0$

```{r}
# Reject or fail to reject?
summary(model1)$coeff
# Since p-value is less than alpha, reject the null hypothesis
```

**5a, 5b, 5c, 5d, 5e**

**Solutions say that the residuals are not normally distributed which is wrong**
**Therefore the regression analysis is reliable**

```{r}
# Yes, the plot is linear
ggplot(MLB_Pitchers, aes(ERA, Wins)) + geom_point(col = "steelblue")

# Residual Plot shows no obvious sign of patterns
# Residual plot looks constant across all values of the independent variable
ggplot(model1) + geom_point(aes(x = .fitted, y = .resid))
qqnorm(MLB_Pitchers$Wins)
ggplot(MLB_Pitchers, aes(x = Wins)) + geom_bar()
# Clearly we have approximately normally distributed data

# Residual normality
ggplot(MLB_Pitchers, aes(x = model1$residuals)) + geom_histogram(bins = 15)
qqnorm(model1$residuals)

# It appears the residuals are approximately normally distributed
```


**Myrtle Beach Homes**

**1a, 1b, 1c, 1d**

**Solutions show that there is a variable called "bedrooms" but this is not correct**

```{r}
colnames(Myrtle_Beach_Homes)[5] <- "SquareFeet"
# Get correlation matrix of numeric variables (quantitative variables)
sapply(Myrtle_Beach_Homes, is.numeric)
cor(Myrtle_Beach_Homes[, which(sapply(Myrtle_Beach_Homes, is.numeric)), with = FALSE])

# Get the max correlation with our response variable "Wins"
correl <- cor(Myrtle_Beach_Homes[, which(sapply(Myrtle_Beach_Homes, is.numeric)), with = FALSE])

# Drop perfects
correl[correl == 1] <- NA

# Melt correl
correl <- na.omit(melt(correl))
correl_table <- setDT(correl[order(-abs(correl$value)),])
correl_table[X1 == "Selling Price ($000)" | X2 == "Selling Price ($000)"][2]
```

$y=\beta_0 + \beta_1x + \epsilon$

**2a, 2b, 2c**

```{r}
model2 <- lm(`Selling Price ($000)` ~ SquareFeet, data = Myrtle_Beach_Homes)
summary(model2)$coefficients
```

A one unit increase in Square Feet increases the price by .26.

```{r}
cat("R-squared = ", summary(model2)$r.squared * 100, "%")
```

**3a, 3b, 3c**

**I got 635.88 for part a instead of 635.66 like he did in the solutions.**

```{r}
# Point prediction
predict(model2, data.frame(SquareFeet = 2500))

# Confidence interval
predict(model2, data.frame(SquareFeet = 2500), interval = "confidence", level = 0.90)

# Prediction interval
predict(model2, data.frame(SquareFeet = 2500), interval = "prediction", level = 0.90)
```

**4a, 4b, 4c**

```{r}
# Confidence interval for slope
# Does not include 0
confint(model2, "SquareFeet", level = 0.95)
```

$H_0: \beta_1 = 0$

$H_a: \beta_1 \ne 0$

```{r}
# Reject or fail to reject?
summary(model2)$coeff
# Since p-value is less than alpha, reject the null hypothesis
```

**5a, 5b, 5c, 5d, 5e**

**Solutions say that the residuals are not normally distributed which is wrong**
**Also the residuals plot looks incorrect in the solutions.**
**Therefore the regression analysis is reliable**

```{r}
# Yes, the plot is linear
ggplot(Myrtle_Beach_Homes, aes(SquareFeet, `Selling Price ($000)`)) + geom_point(col = "steelblue")

# Residual Plot shows no obvious sign of patterns
# Residual plot looks constant across all values of the independent variable
ggplot(model2) + geom_point(aes(x = .fitted, y = .resid))
qqnorm(Myrtle_Beach_Homes$`Selling Price ($000)`)
ggplot(Myrtle_Beach_Homes, aes(x = `Selling Price ($000)`)) + geom_histogram()
# We have approximately normally distributed data

# Residual normality
ggplot(Myrtle_Beach_Homes, aes(x = model2$residuals)) + geom_histogram(bins = 15)
qqnorm(model2$residuals)

# It appears the residuals are approximately normally distributed
```


**Philadelphia**

**1a, 1b, 1c, 1d**



```{r}
# Get correlation matrix of numeric variables (quantitative variables)
colnames(Philadelphia)[2] <- "CrimeRate"
sapply(Philadelphia, is.numeric)
cor(Philadelphia[, which(sapply(Philadelphia, is.numeric)), with = FALSE])

# Get the max correlation with our response variable "Wins"
correl <- cor(Philadelphia[, which(sapply(Philadelphia, is.numeric)), with = FALSE])

# Drop perfects
correl[correl == 1] <- NA

# Melt correl
correl <- na.omit(melt(correl))
correl_table <- setDT(correl[order(-abs(correl$value)),])
correl_table[X1 == "House Price ($)" | X2 == "House Price ($)"][2]
```


$y=\beta_0 + \beta_1x + \epsilon$

**2a, 2b, 2c**

**It only makes sense that crime rate increasing should decrease house price**
**Therefore, the solutions are incorrect and I believe I have the correct solutions here**

```{r}
model3 <- lm(`House Price ($)` ~ CrimeRate, data = Philadelphia)
summary(model3)$coefficients
summary(model3)

rm_outlier_data <- Philadelphia[-13,]
model3_1 <- lm(`House Price ($)` ~ CrimeRate, data = rm_outlier_data)
```

A one unit increase in Crime Rate decreases the price by 567.7.

**Solutions have incorrect R^2**

```{r}
cat("R-squared = ", summary(model3)$r.squared * 100, "%")
cat("R-squared = ", summary(model3_1)$r.squared * 100, "%")
```

**3a, 3b, 3c**

```{r}
# Point prediction
predict(model3, data.frame(CrimeRate = 50))

# Confidence interval
predict(model3, data.frame(CrimeRate = 50), interval = "confidence", level = 0.90)

# Prediction interval
predict(model3, data.frame(CrimeRate = 50), interval = "prediction", level = 0.90)
```

**4a, 4b, 4c**

```{r}
# Confidence interval for slope
# Does not include 0
confint(model3, "CrimeRate", level = 0.95)
```

$H_0: \beta_1 = 0$

$H_a: \beta_1 \ne 0$

```{r}
# Reject or fail to reject?
summary(model3)$coeff
# Since p-value is less than alpha, reject the null hypothesis
```

**5a, 5b, 5c, 5d, 5e**


```{r}
# Yes, the plot is linear
ggplot(Philadelphia, aes(CrimeRate, `House Price ($)`)) + geom_point(col = "steelblue")

# Residual Plot shows no obvious sign of patterns
# Residual plot looks constant across all values of the independent variable
ggplot(model3) + geom_point(aes(x = .fitted, y = .resid))
qqnorm(Philadelphia$`House Price ($)`)
ggplot(Philadelphia, aes(x = `House Price ($)`)) + geom_histogram()
# We have approximately normally distributed data

# Residual normality
ggplot(Philadelphia, aes(x = model3$residuals)) + geom_histogram(bins = 15)
qqnorm(model3$residuals)

# It appears the residuals are approximately normally distributed with a slight skew
```

**The Philadelphia solutions are wrong on a lot of places.  I'm not sure what happened.**

**Production Costs**

**1a, 1b, 1c, 1d**



```{r}
# Get correlation matrix of numeric variables (quantitative variables)
colnames(Production_Costs) <- c("AvgCost", "MatCost", "LabHours", "MacHours", "Holes")
sapply(Production_Costs, is.numeric)
cor(Production_Costs[, which(sapply(Production_Costs, is.numeric)), with = FALSE])

# Get the max correlation with our response variable "Wins"
correl <- cor(Production_Costs[, which(sapply(Production_Costs, is.numeric)), with = FALSE])

# Drop perfects
correl[correl == 1] <- NA

# Melt correl
correl <- na.omit(melt(correl))
correl_table <- setDT(correl[order(-abs(correl$value)),])
correl_table[X1 == "AvgCost" | X2 == "AvgCost"][2]
```


$y=\beta_0 + \beta_1x + \epsilon$

**2a, 2b, 2c**

```{r}
model4 <- lm(AvgCost ~ LabHours, data = Production_Costs)
summary(model4)$coefficients
```

A one unit increase in Crime Rate decreases the price by 567.7.

**Solutions have incorrect R^2**

```{r}
cat("R-squared = ", summary(model4)$r.squared * 100, "%")
```

**3a, 3b, 3c**

```{r}
# Point prediction
predict(model4, data.frame(LabHours = .521))

# Confidence interval
predict(model4, data.frame(LabHours = .521), interval = "confidence", level = 0.90)

# Prediction interval
predict(model4, data.frame(LabHours = .521), interval = "prediction", level = 0.90)
```

**4a, 4b, 4c**

```{r}
# Confidence interval for slope
# Does not include 0
confint(model4, "LabHours", level = 0.95)
```

$H_0: \beta_1 = 0$

$H_a: \beta_1 \ne 0$

```{r}
# Reject or fail to reject?
summary(model4)$coeff
# Since p-value is less than alpha, reject the null hypothesis
```

**5a, 5b, 5c, 5d, 5e**


```{r}
# Yes, the plot is linear
ggplot(Production_Costs, aes(LabHours, AvgCost)) + geom_point(col = "steelblue")

# Residual Plot shows no obvious sign of patterns
# Residual plot looks constant across all values of the independent variable
ggplot(model4) + geom_point(aes(x = .fitted, y = .resid))
qqnorm(Production_Costs$AvgCost)
ggplot(Production_Costs, aes(x = AvgCost)) + geom_histogram()
# We have approximately normally distributed data

# Residual normality
ggplot(Production_Costs, aes(x = model4$residuals)) + geom_histogram(bins = 15)
qqnorm(model4$residuals)

# It appears the residuals are approximately normally distributed with a slight skew
```

**Residual Car Value**

**1a, 1b, 1c, 1d**



```{r}
# Get correlation matrix of numeric variables (quantitative variables)
sapply(Residual_Car_Values, is.numeric)
cor(Residual_Car_Values[, which(sapply(Residual_Car_Values, is.numeric)), with = FALSE])

# Get the max correlation with our response variable "Wins"
correl <- cor(Residual_Car_Values[, which(sapply(Residual_Car_Values, is.numeric)), with = FALSE])

# Drop perfects
correl[correl == 1] <- NA

# Melt correl
correl <- na.omit(melt(correl))
correl_table <- setDT(correl[order(-abs(correl$value)),])
correl_table[X1 == "Price" | X2 == "Price"][2]
```


$y=\beta_0 + \beta_1x + \epsilon$

**2a, 2b, 2c**

```{r}
model5 <- lm(Price ~ Mileage, data = Residual_Car_Values)
summary(model5)$coefficients
```

A one unit increase in Crime Rate decreases the price by 567.7.

```{r}
cat("R-squared = ", summary(model5)$r.squared * 100, "%")
```

**3a, 3b, 3c**

```{r}
# Point prediction
predict(model5, data.frame(Mileage = 15000))

# Confidence interval
predict(model5, data.frame(Mileage = 15000), interval = "confidence", level = 0.95)

# Prediction interval
predict(model5, data.frame(Mileage = 15000), interval = "prediction", level = 0.95)
```

**4a, 4b, 4c**

```{r}
# Confidence interval for slope
# Does not include 0
confint(model5, "Mileage", level = 0.95)
```

$H_0: \beta_1 = 0$

$H_a: \beta_1 \ne 0$

```{r}
# Reject or fail to reject?
summary(model5)$coeff
# Since p-value is less than alpha, reject the null hypothesis
```

**5a, 5b, 5c, 5d, 5e**


```{r}
# Yes, the plot is linear
ggplot(Residual_Car_Values, aes(Mileage, Price)) + geom_point(col = "steelblue")

# Residual Plot shows no obvious sign of patterns
# Residual plot looks constant across all values of the independent variable
ggplot(model5) + geom_point(aes(x = .fitted, y = .resid), col = "steelblue")
qqnorm(Residual_Car_Values$Price)
ggplot(Residual_Car_Values, aes(x = Price)) + geom_histogram(col = "steelblue", fill = "steelblue", bins = 20)
# We have approximately normally distributed data

# Residual normality
ggplot(Residual_Car_Values, aes(x = model5$residuals)) + geom_histogram(col = "steelblue", fill = "steelblue", bins = 15)
qqnorm(model5$residuals)

# It appears the residuals are approximately normally distributed with a slight skew
```

